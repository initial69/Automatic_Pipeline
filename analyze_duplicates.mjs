#!/usr/bin/env node

import { readFileSync, existsSync } from 'node:fs';
import path from 'node:path';

/**
 * Analyze Duplicates Script
 * 
 * This script analyzes the current deduplication tracker to identify
 * potential duplicate content and provides insights.
 */

const dedupFile = path.join('data', 'deduplication_tracker.json');

function analyzeDuplicates() {
  console.log('üîç Analyzing Duplicate Content...');
  console.log('==================================');
  
  if (!existsSync(dedupFile)) {
    console.log('‚ùå No deduplication tracker found. Run the pipeline first.');
    return;
  }
  
  try {
    const tracker = JSON.parse(readFileSync(dedupFile, 'utf8'));
    
    console.log(`üìä Deduplication Tracker Analysis`);
    console.log(`üìÖ Last Updated: ${tracker.lastUpdated}`);
    console.log(`üîÑ Reset Reason: ${tracker.resetReason || 'N/A'}`);
    console.log(`üîÑ Reset Timestamp: ${tracker.resetTimestamp || 'N/A'}`);
    
    // Analyze published items
    const publishedItems = tracker.published || {};
    const publishedCount = Object.keys(publishedItems).length;
    
    console.log(`\nüì§ Published Items: ${publishedCount}`);
    
    if (publishedCount > 0) {
      // Group by source
      const sourceGroups = {};
      const urlSet = new Set();
      const duplicateUrls = [];
      
      for (const [key, item] of Object.entries(publishedItems)) {
        const source = item.source || 'Unknown';
        if (!sourceGroups[source]) {
          sourceGroups[source] = [];
        }
        sourceGroups[source].push(item);
        
        // Check for duplicate URLs
        if (item.link) {
          const cleanUrl = item.link.split('?')[0].split('#')[0].toLowerCase();
          if (urlSet.has(cleanUrl)) {
            duplicateUrls.push({
              url: cleanUrl,
              original: item,
              key: key
            });
          } else {
            urlSet.add(cleanUrl);
          }
        }
      }
      
      console.log(`\nüìä Sources with most content:`);
      const sortedSources = Object.entries(sourceGroups)
        .sort(([,a], [,b]) => b.length - a.length)
        .slice(0, 10);
      
      sortedSources.forEach(([source, items]) => {
        console.log(`   ${source}: ${items.length} items`);
      });
      
      // Check for duplicate URLs
      if (duplicateUrls.length > 0) {
        console.log(`\nüö® DUPLICATE URLs FOUND: ${duplicateUrls.length}`);
        console.log('=====================================');
        duplicateUrls.forEach((dup, index) => {
          console.log(`${index + 1}. URL: ${dup.url}`);
          console.log(`   Source: ${dup.original.source}`);
          console.log(`   Title: ${dup.original.title}`);
          console.log(`   Timestamp: ${dup.original.timestamp}`);
          console.log('');
        });
      } else {
        console.log(`\n‚úÖ No duplicate URLs found in published items`);
      }
      
      // Check for recent activity
      const now = new Date();
      const oneHourAgo = new Date(now.getTime() - 60 * 60 * 1000);
      const oneDayAgo = new Date(now.getTime() - 24 * 60 * 60 * 1000);
      
      let recentCount = 0;
      let dailyCount = 0;
      
      for (const item of Object.values(publishedItems)) {
        if (item.timestamp) {
          const itemTime = new Date(item.timestamp);
          if (itemTime > oneHourAgo) recentCount++;
          if (itemTime > oneDayAgo) dailyCount++;
        }
      }
      
      console.log(`\n‚è∞ Recent Activity:`);
      console.log(`   Last hour: ${recentCount} items`);
      console.log(`   Last 24 hours: ${dailyCount} items`);
    }
    
    // Analyze content hashes
    const contentHashes = tracker.content_hashes || {};
    const contentHashesCount = Object.keys(contentHashes).length;
    
    console.log(`\nüî§ Content Hashes: ${contentHashesCount}`);
    
    // Analyze title hashes
    const titleHashes = tracker.title_hashes || {};
    const titleHashesCount = Object.keys(titleHashes).length;
    
    console.log(`\nüìù Title Hashes: ${titleHashesCount}`);
    
    // Analyze source frequency
    const sourceHashes = tracker.source_hashes || {};
    const sourceCount = Object.keys(sourceHashes).length;
    
    console.log(`\nüì° Source Frequency Tracking: ${sourceCount} sources`);
    
    if (sourceCount > 0) {
      console.log(`\nüìä Source Activity (last 24h):`);
      const now = new Date();
      const oneDayAgo = new Date(now.getTime() - 24 * 60 * 60 * 1000);
      
      for (const [source, timestamps] of Object.entries(sourceHashes)) {
        const recentTimestamps = timestamps.filter(ts => new Date(ts) > oneDayAgo);
        if (recentTimestamps.length > 0) {
          console.log(`   ${source}: ${recentTimestamps.length} posts in last 24h`);
        }
      }
    }
    
    // Recommendations
    console.log(`\nüí° Recommendations:`);
    console.log('==================');
    
    if (duplicateUrls.length > 0) {
      console.log(`üö® CRITICAL: Found ${duplicateUrls.length} duplicate URLs`);
      console.log(`   - Run: node reset_deduplication.mjs`);
      console.log(`   - This will clear the tracker and prevent future duplicates`);
    }
    
    if (publishedCount > 1000) {
      console.log(`‚ö†Ô∏è  Large tracker size (${publishedCount} items)`);
      console.log(`   - Consider periodic cleanup of old entries`);
    }
    
    if (recentCount > 50) {
      console.log(`‚ö†Ô∏è  High recent activity (${recentCount} items in last hour)`);
      console.log(`   - Consider reducing pipeline frequency`);
    }
    
    console.log(`‚úÖ Analysis completed successfully`);
    
  } catch (error) {
    console.error(`‚ùå Error analyzing tracker: ${error.message}`);
    process.exit(1);
  }
}

// Run if called directly
if (process.argv[1] && process.argv[1].endsWith('analyze_duplicates.mjs')) {
  analyzeDuplicates();
}

export { analyzeDuplicates };
